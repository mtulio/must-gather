#!/bin/bash

# Gather Monitoring Metrics and Dashboards.
# - Discovery and save Grafana dashboards, extracting the metrics' datapoints from Prometheus.
# - Gather custom metrics (when defined on Config variables).
# - Discovery metrics from prefix name.
#
# Config variables:
# - GAHTER_MONIT_START_DATE: starting human readable date string (date -d). Default: "7 days ago".
# - GAHTER_MONIT_END_DATE: ending human readable date string (date -d). Default: "now".
# - GAHTER_MONIT_QUERY_STEP: metric resolution to get from Prometheus. Default: "1m".
# - GATHER_MONIT_CUSTOM_METRICS: list of additional metrics name to be collected. Default: Undefined
# - GATHER_MONIT_DISCOVERY_GRAFANA: manage Grafana metrics discovery, 'no' is disabled. Default: != 'no'
# - GATHER_MONIT_DISCOVERY_PREFIXES: discovery metrics by prefixes. Default: ''
# - GAHTER_MONIT_TOKEN: optional service account token to gather Prometheus and Grafana API. Default: Undefined
#
# To create the ConfigMap:
# $ echo -e 'GAHTER_MONIT_START_DATE="15 days ago"\nGAHTER_MONIT_QUERY_STEP="5m"' > env
# $ echo -e 'GATHER_MONIT_CUSTOM_METRICS="up"' > env
# $ echo -e 'GATHER_MONIT_DISCOVERY_GRAFANA="no"\nGATHER_MONIT_CUSTOM_METRICS="up"' > env
# $ echo -e 'GATHER_MONIT_DISCOVERY_PREFIXES="apiserver_"' > env
# $ oc create configmap must-gather-env -n openshift-monitoring --from-file=env=env
#
# References:
# - Prometheus API: https://prometheus.io/docs/prometheus/latest/querying/api/
# - Grafana API: https://grafana.com/docs/grafana/latest/http_api/dashboard/

#Safeguards
set -o pipefail

err_report() {
    echo "ERROR: on line $1 from ${0}"
}
trap 'err_report $LINENO' ERR

GAHTER_MONIT_START_DATE_DEFAULT="7 days ago"
GAHTER_MONIT_END_DATE_DEFAULT="now"
GAHTER_MONIT_QUERY_STEP_DEFAULT="1m"
GATHER_MONIT_DISCOVERY_GRAFANA_DEFAULT="yes"

MONIT_NS="openshift-monitoring"
MONIT_ENV="must-gather-env"

JQ_PATH=$(command -v jq 2>/dev/null)
CURL_CMD=(curl --fail -sk)

BASE_COLLECTION_PATH="/must-gather"
MONITORING_PATH="${BASE_COLLECTION_PATH}/monitoring"
PROM_PATH="${MONITORING_PATH}/prometheus"
GF_PATH="${MONITORING_PATH}/grafana"

test -d "${PROM_PATH}" || mkdir -p "${PROM_PATH}"
test -d "${GF_PATH}" || mkdir -p "${GF_PATH}"

echo "INFO: Get custom metrics from ConfigMap ${MONIT_ENV} on project ${MONIT_NS}"
oc -n "${MONIT_NS}" \
    get cm "${MONIT_ENV}" \
    -o jsonpath='{.data.env}' > "${MONITORING_PATH}/env" 2> /dev/null
if [[ $? -eq 0 ]]; then
    echo "INFO: Loading custom environments variables from ${MONITORING_PATH}/env"
    source "${MONITORING_PATH}/env"
else
    echo "INFO: Unable to load custom environments from ConfigMap, ignoring."
fi

#Session token, overwriten by GAHTER_MONIT_TOKEN
OCP_TOKEN="${GAHTER_MONIT_TOKEN:-$(oc whoami -t)}"
SA_TOKEN="$(oc sa get-token default)"

# this is a CA bundle we need to verify the routes
oc -n openshift-config-managed \
    get cm default-ingress-cert \
    -o jsonpath='{.data.ca-bundle\.crt}' > "${MONITORING_PATH}/ca-bundle.crt"

# Timestamp to Prometheus query time range
_START_HUNAN=${GAHTER_MONIT_START_DATE:-${GAHTER_MONIT_START_DATE_DEFAULT}}
DATE_START=$(date -d "${_START}" +%s)
_END_HUMAN=${GAHTER_MONIT_END_DATE:-${GAHTER_MONIT_END_DATE_DEFAULT}}
DATE_END=$(date -d "${_END_HUMAN}" +%s)

# Metric resolution: If the resolution is too low, the API will deny due to 11k limitation
QUERY_STEP=${GAHTER_MONIT_QUERY_STEP:-${GAHTER_MONIT_QUERY_STEP_DEFAULT}}

PROM_HOST=$(oc -n "${MONIT_NS}" get route prometheus-k8s -o jsonpath='{.spec.host}{"\n"}')
PROM_URL="https://${PROM_HOST}"

GF_HOST=$(oc -n "${MONIT_NS}" get route grafana -o jsonpath='{.spec.host}{"\n"}')
GF_URL="https://${GF_HOST}"

echo "INFO: Metrics config time range from=${DATE_START} to=${DATE_END} step=${QUERY_STEP}"
echo "INFO: Config Env GAHTER_MONIT_START_DATE: ${GAHTER_MONIT_START_DATE}"
echo "INFO: Config Env GAHTER_MONIT_END_DATE: ${GAHTER_MONIT_END_DATE}"
echo "INFO: Config Env GAHTER_MONIT_QUERY_STEP: ${GAHTER_MONIT_QUERY_STEP}"
echo "INFO: Config Env GATHER_MONIT_CUSTOM_METRICS: ${GATHER_MONIT_CUSTOM_METRICS}"
echo "INFO: Config Env GATHER_MONIT_DISCOVERY_PREFIXES: ${GATHER_MONIT_DISCOVERY_PREFIXES}"
echo "INFO: Config Env GATHER_MONIT_DISCOVERY_GRAFANA: ${GATHER_MONIT_DISCOVERY_GRAFANA}"
echo "INFO: Prometheus endpoint URL: ${PROM_URL}"
echo "INFO: Grafana endpoint URL: ${GF_URL}"

function install_jq() {

    if [[ -f ${JQ_PATH} ]]; then
        return
    fi

    # install jq from binary (required to Dashnoard parser)
    export JQ_PATH=/usr/local/bin/jq
    "${CURL_CMD[@]}" -o "${JQ_PATH}" \
      http://stedolan.github.io/jq/download/linux64/jq && \
      chmod +x "${JQ_PATH}"
}

# Make a query (single metric) to Prometheus API /api/v1/query_range
function prom_query_range() {
    QUERY=$1
    METRIC_FILE=${2:-$(echo "${QUERY}" |awk -F'\(' '{print$1}')}

    # The metrics are ziped to decrease the datatransfer
    "${CURL_CMD[@]}" \
        -H "Authorization: Bearer ${OCP_TOKEN}" \
        -H "Accept-encoding: gzip" \
        --data-urlencode "query=${QUERY}" \
        --data-urlencode "start=${DATE_START}" \
        --data-urlencode "end=${DATE_END}" \
        --data-urlencode "step=${QUERY_STEP}" \
        "${PROM_URL}/api/v1/query_range" > "${PROM_PATH}/metric-${METRIC_FILE}.json.gz"
}

# Gather metrics defined on environment GATHER_MONIT_CUSTOM_METRICS
function get_custom_metrics() {

    if [[ -z "${GATHER_MONIT_CUSTOM_METRICS}" ]]; then
        return
    fi

    echo "INFO: Metrics will be collected from [$(date -d "@${DATE_START}")] to [$(date -d "@${DATE_END}")]"
    for METRIC in ${GATHER_MONIT_CUSTOM_METRICS}; do
        echo "INFO: Getting metric range: ${METRIC}"
        prom_query_range "${METRIC}" "${METRIC}"
    done
}

# Discovery metrics from prefixes defined on environment GATHER_MONIT_DISCOVERY_PREFIXES
function get_discovery_metrics() {

    if [[ -z "${GATHER_MONIT_DISCOVERY_PREFIXES}" ]]; then
        return
    fi

    echo "INFO: Discovery all metrics' name and save on ${MONITORING_PATH}/prometheus-metrics.txt"
    "${CURL_CMD[@]}" \
        -H "Authorization: Bearer ${OCP_TOKEN}" \
        -H "Accept-encoding: gzip" \
        -g "${PROM_URL}/api/v1/label/__name__/values" \
        | gunzip \
        | jq -r '.data[]' > "${MONITORING_PATH}/prometheus-metrics.txt"

    echo "INFO: Starting discovery metrics by prefixes..."
    # TODO: fix it O(m*n) ;(
    while read METRIC; do
        for PREFIX in ${GATHER_MONIT_DISCOVERY_PREFIXES}; do
            if [[ "${METRIC}" =~ ^${PREFIX}.* ]]; then
                echo "INFO: Getting metric range: ${METRIC}"
                prom_query_range "${METRIC}" "${METRIC}" ;
            fi
        done
    done < "${MONITORING_PATH}/prometheus-metrics.txt"
}

# List all Dashboards from default folder (id=1) on Grafana - where OCP store it's dashboards
function gf_discovery_dashboards() {

    echo "INFO: Discovery Default Grafana Dashboard folder: ${GF_PATH}/dashboards.json"
    "${CURL_CMD[@]}" \
        -H "Authorization: Bearer ${OCP_TOKEN}" \
        "${GF_URL}/api/search?folderIds=1" > "${GF_PATH}/dashboards.json"
}

# Parse and download all Dashboards discovered from Default folder,
# then collect it's metrics from Prometheus API.
function gf_discovery_metrics_from_dashboards() {

    install_jq
    echo -ne "INFO: Extract discovered dashboards: ${GF_PATH}/dashboards.txt"
    ${JQ_PATH} \
        -r '.[] | .uri +":"+ .uid' ${GF_PATH}/dashboards.json \
        | awk -F'db/' '{print$2}' \
        | awk '{print$1":"$2}' > ${GF_PATH}/dashboards.txt
    echo " [$(wc -l ${GF_PATH}/dashboards.txt |awk '{print$1}')]"

    while read -r DASHBOARD; do
        NAME=$(echo "${DASHBOARD}" |awk -F':' '{print$1}')
        DUID=$(echo "${DASHBOARD}" |awk -F':' '{print$2}')

        echo "INFO: Getting Dashboard ${NAME}"
        "${CURL_CMD[@]}" \
            -H "Authorization: Bearer ${OCP_TOKEN}" \
            "${GF_URL}/api/dashboards/uid/${DUID}" > "${GF_PATH}/dashboard_${NAME}.json"

        ${JQ_PATH} \
            -r \'.dashboard.rows[].panels[].targets[].expr | match("\\(([\\w:]+){.*").captures[].string \' \
            "${GF_PATH}/dashboard_${NAME}.json" > "${GF_PATH}/dashboard_${NAME}_metrics.txt"
    done < "${GF_PATH}/dashboards.txt"

    echo -ne "INFO: Grouping all dashboards' metrics to ${GF_PATH}/dashboards_metrics.txt"
    sort -u ${GF_PATH}/dashboard_*_metrics.txt > "${GF_PATH}/dashboards_metrics.txt"
    echo " [$(wc -l ${GF_PATH}/dashboards_metrics.txt |awk '{print$1}')]"

    echo "INFO: Gathering discovered metrics' data points"
    while read -r METRIC; do
        echo "INFO: Getting metric range: ${METRIC}"
        prom_query_range "${METRIC}" "${METRIC}"
    done < "${GF_PATH}/dashboards_metrics.txt"
}

function get_grafana_metrics() {

    ENABLE_DISCOVERY=${GATHER_MONIT_DISCOVERY_GRAFANA:-${GATHER_MONIT_DISCOVERY_GRAFANA_DEFAULT}}
    if [[ "${ENABLE_DISCOVERY}" == "no" ]]; then
        echo "INFO: Ignoring metrics from Grafana dashboards. GATHER_MONIT_DISCOVERY_GRAFANA=no"
        return
    fi

    echo "INFO: Get metrics from Grafana dashboards"
    gf_discovery_dashboards
    gf_discovery_metrics_from_dashboards
}


function get_alerts() {

    set -o nounset
    set -o errexit

    # using oc get --raw because we directly control it and have standardized debugging on it
    oc get \
        --server="${PROM_URL}" \
        --token="${SA_TOKEN}" \
        --certificate-authority="${MONITORING_PATH}/ca-bundle.crt" \
        --raw=/api/v1/rules?type=alert 2>"${MONITORING_PATH}/alert.stderr" > "${MONITORING_PATH}/alerts.json"

    rm "${MONITORING_PATH}/ca-bundle.crt"
}

#> Main
# Get custom metrics custom
get_custom_metrics

# Get Grafana dashboards and it's metrics data points
get_grafana_metrics

# Get discovery metrics
get_discovery_metrics

# Get Prometheus Alerts
get_alerts

# force disk flush to ensure that all data gathered is accessible in the copy container
echo "INFO: Must-gather monitoring finished"
sync
